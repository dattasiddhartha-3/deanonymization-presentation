<html>
<head>
    <meta http-equiv="X-UA-Compatible" content="IE=EmulateIE11"/>
    <link rel="stylesheet" href="reveal.js/css/reveal.css">
    <link rel="stylesheet" href="reveal.js/css/theme/black.css">
    <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css" />
    <link rel="stylesheet" href="reveal.js/css/custom-gc.css" />
</head>
<body>
<div class="reveal">
<div class="slides">

<section class="fig-container"
         data-file="d3-fig/eclipses.html" data-no-background>
    <div class='fancy-border' style="position: absolute; width:80%; left: 100px; top: 250px">
        Graph Deanonymization: Circumventing Privacy in Anonymized Networks
    </div>
    <p style="font-size: 30px; position: absolute; margin-left: 40%; top: 550px">
        <i>Siddhartha Datta</i>
    </p>
    <p style="font-size: 10px; position: absolute; margin-left: 38%; top: 600px">
        (Press right arrow or space bar to navigate to next slide)
    </p>
</section>
    
<section>
    <h3>Anonymization</h3>
    <div class="row no-margin-top">
        <div class="col-50 no-margin-top left" style="width:75%;">
            <p class="small">What do we wish to achieve from network anonymization? Why do networks need to be anonymous? What are our constraints?</p>
            <ul>
                <li class="small">Share the network data (e.g. for academic or industrial collaboration)</li>
                <li class="small">Domain may be sensitive (e.g. communication data, social networks, mobility trace networks, peer-to-peer networks, epidemiological/healthcare networks)</li>
                <li class="small">Things we may wish to hide: node privacy, edge privacy, </li>
                <li class="small">Preserve graph properties (e.g. Degree, Path Length, Local & Global Clustering Coefficients, Betweenness Centrality)</li>
                <li class="small">Inhibit de-anonymization</li>
            </ul>
        </div>
    </div>
</section>
    
<section>
  <h3>Controversey over the Netflix Prize dataset</h3>
  <div class="row" style="margin-top: 0">
      <div class="col-100">
        <p>Netflix released an pseudonymized dataset of movie preferences, as part of a competition into recommender systems. Researchers use information from the Internet Movie Data Base (IMDB) to de-anonymize a number of records. After 2009 the completion is suspended following a lawsuit on privacy grounds.</p>
        <div class="fig-container" style="height: 400px;"
         data-file="https://paulovn.github.io/movie-network/" 
         data-scrollable="yes">
        </div>  
      </div>
  </div>
</section>
    
<section>
    <h3>De-anonymization</h3>
    <div class="row no-margin-top">
        <div class="col-50 no-margin-top left">
            <ul>
                <li class="small">Incentives: learn sensitive patterns amongst node attributes, learn sensitive attributes about nodes</li>
                <li class="small">Attacks that help applications: mass surveilance, abusive marketing, phishing</li>
            </ul>
        </div>
    </div>
</section>
  
<section>
    <h3>Methods</h3>
    <div class="row no-margin-top">
        <div class="col-50 no-margin-top">
            <p>Anonymization Techniques:</p>
            <ul>
                <li class="small">Naive ID Removal: replacing node label with randomly-mapped node label</li>
                <li class="small">Edge Editing based Anonymization: <i>AddDel</i> (k randomly chosen edges will be added to G followed by another k randomly chosen edges will be deleted from G), <i>Switch</i> (k edge switches are conducted, where edges between nodes will be removed and re-routed to another set of nodes)</li>
                <li class="small">k-anonymity: remove node attributes based on metric, e.g. k-Neighborhood Anonymity, k-Degree Anonymity</li>
                <li class="small">Aggregation/Class/Cluster based Anonymization: anonymize users into clusters; anonymized graph should consist of supernodes (each corresponding to nodes in a cluster/subgraph), and superedges (edge densities among supernodes)</li>
                <li class="small">Random Walk based Anonymization: an edge between two users i and j is replaced by another edge between i and u, where u is the destination of a random walk starting from j</li>
            </ul>
            <p>De-anonymization Techniques:</p>
            <ul>
                <li class="small">Seed-based De-anonymization: identify some users as seeds first (e.g. infiltrate a social network and identify your seed node subgraph); active (e.g. create extensive fake accounts around network), passive (e.g. existing personal account)</li>
                <li class="small">Seed-free De-anonymization: use probablistic reasoning (e.g. minimizing edge difference between anonymized graph and auxiliary graph with simulated node labelleing and simulated edge creation/removal)</li>
                <li class="small">Seed-based/Active De-anonymization: </li>
                <li class="small">Seed-free/Passive De-anonymization: </li>
            </ul>
            <p>De-anonymization attack components:</p>
            <ul>
                <li class="small">Seed: an attacker's unique insertion that can help them reverse whatever anonymizes their seed</li>
                <li class="small">Active/Passive: whether the attacker needs to make widespread modifications to their network they wish to compromise</li>
                <li class="small">Auxiliary graph: any pre-existing data that can be used in conjunction with the anonymized graph to match patterns / identify nodes, etc</li>
                <li class="small">Domain knowledge: if the attacker can estimate the scheme used to anonymize the dataset (e.g. based on community), then they can reformulate their de-anonymization strategy</li>
                <li class="small"></li>
            </ul>
        </div>
    </div>
</section>
            
<section class="fig-container"
         data-file="https://dattasiddhartha-3.github.io/deanonymization-presentation/networkx/data_load.html">
    <div style="width: 50%; margin-left: 45% !important; background-color: #777777; font-size: 0.65em; border-radius: 20px;">
         <p class="small">Hypothetical situation:</p>
        <ul>
            <li class="small">The TAs are provided with a k-anonymized graph dataset of student summatives (i.e. some nodes are anonymized, some attributes are anonymized). Is it beyond the realm of reason to expect them to figure out who wrote what?</li>
            <li class="small">Imagine Bernie takes each student and pairs them to their summative text.</li>
            <li class="small">To restrict their access to the full text, he may process each sentence into a set of attributes (e.g. 100 most frequently used words in the summative).</li>
        </ul>
    </div>
</section>
    
<section class="fig-container"
         data-file="https://dattasiddhartha-3.github.io/deanonymization-presentation/networkx/anonymize.html">
    <div style="width: 50%; margin-left: 45% !important; background-color: #777777; font-size: 0.65em; border-radius: 20px;">
         <p class="small">k-attribute anonmization</p>
    </div>
</section>
        
    
<section class="fig-container"
         data-file="https://dattasiddhartha-3.github.io/deanonymization-presentation/networkx/eval_deanonymize.html">
</section>
     
<section>
    <h3>Evaluation & Reflection</h3>
    <div class="row no-margin-top">
        <div class="col-50 no-margin-top left">
            <p class="small">Should we be concerned? Is the method flawed? What would put our privacy at greater risk? What can we do to "defend" ourselves?</p>
            <ul>
                <li class="small">Structure-based de-anonymization / known node list from other external sources (which the defender cannot control for) nullifies many methods that still retain graph structure</li>
                <li class="small">Perturbing the graph changes its structure, and may nullify the purpose of graph sharing in the first place (e.g. if too many edges removed and random ones created, then clustering coefficients may change drastically)</li>
                <li class="small">De-anonymization is not a fully-automatic process; it requires understanding the context, accumulating resources, identifying vulnerabilities</li>
                <li class="small">The use of seeds in de-anonymization is more reliable, but not all situations may support seeds.</li>
            </ul>
        </div>
        <div class="col-50 no-margin-top">
        <div class="fig-container no-margin-top" data-file="d3-fig/class-deanon-network.html"></div>
    </div>
</section>
    
<section>
    <h5>More papers to read:</h5>
    <div class="row no-margin-top">
        <div class="col-50 no-margin-top left">
            <ul>
                <li class="small"><a href="https://www.cs.cornell.edu/home/kleinber/www07-anon.pdf">Wherefore Art Thou R3579X? Anonymized Social Networks, Hidden Patterns, and Structural Steganography</a></li>
                <li class="small"><a href="https://www.yongyeol.com/papers/nilizadeh-deanon-2014.pdf">Community-Enhanced De-anonymization of Online Social Networks</a></li>
                <li class="small"><a href="https://www.cs.cornell.edu/~shmat/shmat_oak09.pdf">De-anonymizing Social Networks</a></li>
                <li class="small"><a href="http://palms.ee.princeton.edu/system/files/Blind+De-anonymization+Attacks+using+Social+Networks.pdf">Blind De-anonymization Attacks using Social Networks</a></li>
                <li class="small"><a href="http://www0.cs.ucl.ac.uk/staff/G.Danezis/papers/wpes24f-sharad_danezis.pdf">An Automated Social Graph De-anonymization Technique</a></li>
            </ul>
        </div>
    </div>
</section>
    

    
</div> <!-- slides -->
</div> <!-- Reveal -->

<script src="reveal.js/js/reveal.js"></script>
<script>

// Full list of configuration options available at:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
    controls: false,
    progress: true,
    history: true,
    center: false,
    transition: 'fade',
    viewDistance: 3, // to lazy load iframes

    // // optional configurations for reveald3
    reveald3: {

         // If the previous slide is a slide further in the deck (i.e. we come back to
         // slide from the next slide), by default the last fragment transition will be
         // triggered to to get the last state of the visualization and simulate the
         // the state the simulation was in when we left the slide. This can be
         // discarded.
         runLastState: true, //default true

         // If a special onSlideChanged transition has been set (if for example the visualization
         // has been preloaded using the data-preload attribute, and you want a specific transition
         // to happen only when you arrive on the slide), you can choose the delay with which such 
         // a transition will occur (note that in the case of no data-preload, if no delay is set then
         // the transition might not occur since the iframe might not be fully loaded yet when the 
         // function is triggered). // default 0. 
         onSlideChangedDelay: 400,

         // This will prefix the path attributes of the source html paths with the given path.
         // (by default "src" if set to true or with the specified path if string)
         mapPath: false, // default: false

         // If true, will try to locate the file at a fallback url without the mapPath prefix in case no file is found
         // at the stipulated url with mapPath
         tryFallbackURL: false, //default false

         // Checking for file existance has been reported to fail in rare 
         // cases though files did exist. This option is to disable the file checking.
         //see: https://github.com/gcalmettes/reveal.js-d3/issues/10
         disableCheckFile: false, //default false,
    },

    // Reveal.js plugins
    dependencies: [

        { src: './reveald3.js' },

    ]
});

</script>
</body>
</html>
